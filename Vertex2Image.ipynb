{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Connect to kernel\n"]}],"source":["print(\"Connect to kernel\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6Qv2uM3sitqN"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Prepare Dataset \"\"\"\n","\n","import numpy as np\n","import json\n","from PIL import Image\n","import glob\n","\n","\n","def get_folder_paths(folder=\"./data/\"):\n","    dataset_folders = []\n","    for path in glob.glob(folder + \"*\"):\n","        dataset_folders.append(path)\n","    dataset_folders = dataset_folders[0:4]\n","    print(\"dataset paths: \", dataset_folders)\n","    return dataset_folders\n","\n","\n","def get_files_path(path):\n","    file_path = []\n","    for file in glob.glob(path):\n","        file_path.append(file)\n","    return file_path\n","\n","\n","def read_image(path):\n","    with Image.open(path, \"r\") as img:\n","        data = np.array(img)\n","    return data\n","\n","\n","def read_vertex(path):\n","    with open(path, \"r\") as jf:\n","        data = json.loads(jf.read())\n","    return data\n","\n","\n","def read_img_generator(paths):\n","  for file_path in glob.glob(paths):\n","    with Image.open(file_path) as img:\n","      img = np.array(img)\n","      img = np.array([img[:,:,0], img[:,:,1], img[:,:,2]])\n","      yield img\n","\n","\n","def read_input_map_generator(path):\n","  with open(path, \"r\") as jf:\n","    data = json.loads(jf.read())\n","  for input in data:\n","    yield input"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669417260754,"user":{"displayName":"Zach Wang","userId":"06792478608194515288"},"user_tz":480},"id":"a09ZBSlE60d6","outputId":"7e355144-4fab-4ca9-e2f5-d408543e2c63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model is loaded to CUDA\n"]}],"source":["\"\"\" Call Model \"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","\n","from projection_utils import get_corresponding_grid\n","from model import Vertex2Image\n","\n","load_pretrained = False\n","checkpoint_path = './checkpoint/epoch0_folder0_frame_124.pt'\n","cuda = True\n","input_channels = 64\n","\n","with open(\"./vertex_init.json\", \"r\") as lvf:\n","    vertex_feature = json.loads(lvf.read())\n","\n","vertex_feature = torch.tensor(vertex_feature)\n","\n","# vertex_feature = torch.rand(input_channels, 6890)\n","\n","# list_vertex_feature = vertex_feature.numpy().tolist()\n","# with open(\"./vertex_init.json\", \"w\") as lvf:\n","#     json.dump(list_vertex_feature, lvf)\n","\n","if cuda:\n","    vertex_feature = vertex_feature.cuda()\n","\n","\n","model = Vertex2Image(vertex_feature, input_channels)\n","if load_pretrained:\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    print(\"Load Checkpoint\")\n","\n","if cuda:\n","    model.cuda()\n","model.train()\n","\n","if cuda:\n","    criterion = nn.L1Loss().cuda()\n","else:\n","    criterion = nn.L1Loss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","if load_pretrained:\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","if cuda:\n","    print(\"Model is loaded to CUDA\")\n","else:\n","    print(\"Model is loaded to CPU\")\n","    \n","\n","# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n","# lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500,800], gamma=0.1)\n","\n","# model.eval()\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1262255,"status":"ok","timestamp":1669943698040,"user":{"displayName":"Zach Wang","userId":"06792478608194515288"},"user_tz":480},"id":"DGmLOuecAB_E","outputId":"158b8b9c-c1e1-4fd8-9ece-b1b597bdbd39"},"outputs":[{"name":"stdout","output_type":"stream","text":["round:  0 loss:  tensor(121.7447, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  4 loss:  tensor(101.8485, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  8 loss:  tensor(91.3917, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  12 loss:  tensor(83.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  16 loss:  tensor(75.9996, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  20 loss:  tensor(70.6098, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  24 loss:  tensor(65.3927, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  28 loss:  tensor(61.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  32 loss:  tensor(57.3802, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  36 loss:  tensor(54.4431, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  40 loss:  tensor(51.7070, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  44 loss:  tensor(49.4702, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  48 loss:  tensor(47.4533, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  52 loss:  tensor(45.8156, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  56 loss:  tensor(44.4439, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  60 loss:  tensor(43.4167, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  64 loss:  tensor(42.8276, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  68 loss:  tensor(42.1604, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  72 loss:  tensor(41.6118, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  76 loss:  tensor(39.2504, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  80 loss:  tensor(37.9599, device='cuda:0', grad_fn=<MeanBackward0>)\n","round:  84 loss:  tensor(35.8809, device='cuda:0', grad_fn=<MeanBackward0>)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 91\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mround: \u001b[39m\u001b[39m\"\u001b[39m, j, \u001b[39m\"\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m\"\u001b[39m, loss)\n\u001b[0;32m     89\u001b[0m total_losses \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m---> 91\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     93\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     95\u001b[0m \u001b[39mif\u001b[39;00m j \u001b[39m==\u001b[39m \u001b[39m124\u001b[39m \u001b[39mor\u001b[39;00m j \u001b[39m==\u001b[39m \u001b[39m248\u001b[39m:\n","File \u001b[1;32mc:\\Projects\\3DHumanConstruction\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n","File \u001b[1;32mc:\\Projects\\3DHumanConstruction\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["batch = 4\n","epoch = 140\n","resolution = (256,256)\n","\n","norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","dataset_folders = get_folder_paths()\n","\n","all_loss = {}\n","# def train():\n","  # with autograd.detect_anomaly():\n","\n","for e in range(epoch):\n","  for i, folder in enumerate(dataset_folders):\n","\n","    # n_files = len(get_files_path(folder + \"/images_256by256/*.png\"))\n","    n_files = 250\n","    \n","    gt_image = read_img_generator(folder + \"/images_256by256/*.png\")\n","    color_map = read_input_map_generator(folder + \"/color_inputs.json\")\n","    vert_rot_map = read_input_map_generator(folder + \"/vert_rot.json\")\n","    \n","    total_losses = 0.\n","    for j in range(0, n_files, batch):\n","      gt_image_batch = []\n","      c_batch = []\n","      v_batch = []\n","\n","      for n in range(batch):\n","        img = next(gt_image, None)\n","        c_map = next(color_map, None)\n","        v_map = next(vert_rot_map, None)\n","        if isinstance(img, type(None)) or isinstance(c_map, type(None)) or isinstance(v_map, type(None)):\n","          break\n","\n","        gt_image_batch.append(img)\n","        c_batch.append(c_map)\n","        v_batch.append(v_map)\n","      \n","      if len(gt_image_batch) != 0:\n","        gt_image_batch = np.array(gt_image_batch)\n","        c_batch = np.array(c_batch)\n","        v_batch = np.array(v_batch)\n","\n","        gt_image_batch = torch.tensor(gt_image_batch, dtype=torch.float32)\n","        c_batch = torch.tensor(c_batch, dtype=torch.float32)\n","        c_batch = norm(c_batch)\n","        inputs = torch.cat((c_batch, torch.tensor(v_batch, dtype=torch.float32)), 1)\n","\n","        if cuda:\n","          gt_image_batch = gt_image_batch.cuda()\n","          inputs = inputs.cuda()\n","\n","        optimizer.zero_grad()\n","        y = model(inputs)\n","\n","        if j % 20 == 0:\n","          \n","          # inp = inputs[0].cpu().detach().numpy()\n","          # inp = vert_batch[0].cpu().numpy()\n","          # img = np.zeros(resolution)\n","          # for r, row in enumerate(inp):\n","          #   for c, col in enumerate(row):\n","          #     if np.isnan(col) == False:\n","          #       img[r][c] = 255\n","          # img = img.astype(np.uint8)\n","          # save_img = Image.fromarray(img).convert(\"L\")\n","          # save_img.save(f\"./output/epoch{e}_folder{i}_frame{j}_input.png\")\n","\n","\n","          img = gt_image_batch[0].detach().cpu().numpy()\n","          cr = np.expand_dims(img[0], axis=2)\n","          cg = np.expand_dims(img[1], axis=2)\n","          cb = np.expand_dims(img[2], axis=2)\n","          img_reshape = np.concatenate((cr,cg,cb), 2)\n","          img_reshape = img_reshape.astype(np.uint8)\n","          save_img = Image.fromarray(img_reshape)\n","          save_img.save(f\"./output/epoch{e}_folder{i}_frame{j}_gt.png\")\n","\n","          pred_img = y[0].detach().cpu().numpy()\n","          cr = np.expand_dims(pred_img[0], axis=2)\n","          cg = np.expand_dims(pred_img[1], axis=2)\n","          cb = np.expand_dims(pred_img[2], axis=2)\n","          img_reshape = np.concatenate((cr,cg,cb), 2)\n","          img_reshape = img_reshape.astype(np.uint8)\n","          save_img = Image.fromarray(img_reshape)\n","          save_img.save(f\"./output/epoch{e}_folder{i}_frame{j}_pred.png\")\n","\n","        loss = criterion(y, gt_image_batch)\n","        print(\"round: \", j, \"loss: \", loss)\n","      \n","        total_losses += loss\n","        \n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if j == 124 or j == 248:\n","          print(\"Weight is saved at the checkpoint:\", j)\n","          torch.save({\n","            'epoch': i,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","          }, f\"./checkpoint/epoch{e}_folder{i}_frame_{j}.pt\")\n","          \n","          name = f\"epoch{e}_folder{i}_frame{j}\"\n","          all_loss[name] = round(float(total_losses) / 31, 5)\n","          total_losses = 0.\n","          # all_loss[name] = round(float(total_losses.cpu()) / 64, 5)\n","          # if j == 124:\n","          #   all_loss[name] = round(float(total_losses.cpu()) / 64, 5)\n","          # else:\n","          #   all_loss[name] = round(float(total_losses.cpu()) / (j / 84 * 21), 5)\n","        \n","\n","    print(\"=====================================\")  \n","    print(\"Total Loss Curve: \")\n","    for k in all_loss:\n","      print(k, all_loss[k])\n","    print(\"\")\n","\n","    # lr_scheduler.step(total_losses)\n","    # print(\"current lr rate: \", lr_scheduler._last_lr)\n","    # print(\"\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOVwYvNdfnbZBzMtN5hThjF","collapsed_sections":["STiXJaXq1t2k","xgFegAbtxbAN","9DklQUwSqMNW","0WUDZRK83Si3","RCS6pFWbaUBM","7AofIIcMjG73","StdKSRnr3kwN","4S7_u8J1DwDk"],"machine_shape":"hm","provenance":[{"file_id":"1nUIGOvjzdpPcDUUOFAjDtRewF4PrGZnY","timestamp":1667092585353},{"file_id":"1I7PXwAQE3Qa_Lv_K40kdhjXRNu1ycwqT","timestamp":1667075252172},{"file_id":"1iMsJ9ggl-2xvupkqyzF3GE4hk5ccNANp","timestamp":1666638117921}]},"gpuClass":"standard","kernelspec":{"display_name":"3DHumanConstruction","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"b97ed2d4a8d7239623537e1716cdf4a0c30442e85d4fe92f0ca320f149e8d0af"}}},"nbformat":4,"nbformat_minor":0}
